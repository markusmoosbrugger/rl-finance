{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Policy gradient algorithm\n",
    "This notebook implements the policy gradient algorithm and applies it to the trading environment. The code partly origins\n",
    "from [spinning-up](https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py) adapted to a continuous action space."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "from torch.optim import Adam\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define neural network for the policy. It is a simple linear model pre-initialized with the linear regression weights.\n",
    "We know from the data analysis that at least 6 interests lags are needed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "\n",
    "    def __init__(self, n_obs, n_acts):\n",
    "        super().__init__()\n",
    "        # define one linear layer where each weight is used for one interest rate\n",
    "        self.linear_interest_rates_layer = nn.Linear(n_obs, 1, bias=False)\n",
    "\n",
    "        # define 1 output for the mean of the action distribution\n",
    "        self.mean_out = nn.Linear(1, n_acts * 1, bias=False)\n",
    "\n",
    "        # init the weights of the layer with the previously calculated coefficients of the linear\n",
    "        # regression\n",
    "        with torch.no_grad():\n",
    "            self.linear_interest_rates_layer.weight.copy_(torch.as_tensor([\n",
    "                [\n",
    "                    0.01703097,\n",
    "                    0.00321324,\n",
    "                    0.0409251,\n",
    "                    0.09984709,\n",
    "                    0.07125695,\n",
    "                    0.56419391\n",
    "                ]\n",
    "            ]))\n",
    "\n",
    "        # set the weight to 1.0\n",
    "        self.mean_out.weight.data.fill_(1.0)\n",
    "\n",
    "        # define a parameter for the standard deviation of the action distribution\n",
    "        # This could be changed to a separate output value of the network.\n",
    "        log_std = -0.5 * np.ones(n_acts, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reduce the dimension of the input to a simple array of [interest rate_1, interest rate_2, ... interest rate_n]\n",
    "        interest_rate_input = torch.flatten(x, 1)\n",
    "\n",
    "        # send the first layer\n",
    "        interest_rate_output = self.linear_interest_rates_layer(interest_rate_input)\n",
    "\n",
    "        # get the mean of the action\n",
    "        mean = self.mean_out(interest_rate_output)\n",
    "\n",
    "        # get the standard deviation of the action\n",
    "        std = torch.exp(self.log_std)\n",
    "\n",
    "        # return the mean and the standard deviation of the action distribution\n",
    "        return mean, std\n",
    "\n",
    "    # make function to compute action distribution\n",
    "    def get_policy(self, obs):\n",
    "        mean, std = self.forward(torch.as_tensor(obs, dtype=torch.float32))\n",
    "        # define a normal distribution with the output of the policy\n",
    "        normal_dist = Normal(mean, std)\n",
    "        return normal_dist\n",
    "\n",
    "    # make action selection function (outputs int actions, sampled from policy)\n",
    "    def get_action(self, obs):\n",
    "        # sample an action from the current action distribution\n",
    "        action = self.get_policy(obs).sample()\n",
    "        return action.clamp(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normal Distribution\n",
    "Our Agent needs to produce continuous actions. The actions will be sampled from a Normal distribution. The parameters of\n",
    "the normal distribution depend on the outcome of the network.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the training loop for optimizing the agent."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(train_env,\n",
    "          eval_env,\n",
    "          agent,\n",
    "          lr=0.01,\n",
    "          epochs=50,\n",
    "          batch_size=6000,\n",
    "          evaluation_interval=100\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Trains a Policy Gradient based agent in a training environment and evaluates on the evaluation environment.\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_env: Training environment\n",
    "    eval_env: Evaluation environment\n",
    "    agent: agent based on neural network\n",
    "    lr: learning rate\n",
    "    epochs: number of epochs for training\n",
    "    batch_size: number of observation/action pairs after which a gradient update should happen\n",
    "    evaluation_interval: number of epochs after which the agent should be tested in the test environment\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    def compute_loss(obs, act, weights):\n",
    "        logp = agent.get_policy(obs).log_prob(act)\n",
    "        return -(logp.sum(axis=-1) * weights).mean()\n",
    "\n",
    "    # make optimizer\n",
    "    optimizer = Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "    def evaluate(_env):\n",
    "\n",
    "        infos = list()\n",
    "        obs = _env.reset()\n",
    "\n",
    "        print(\"Weights of neural network\")\n",
    "        print(agent.linear_interest_rates_layer.weight.data)\n",
    "        print(agent.mean_out.weight.data)\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # act in the environment\n",
    "            act = agent.get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            act_processed = act.numpy()[0]\n",
    "            obs, rew, done, info = _env.step(act_processed)\n",
    "\n",
    "            infos.append(info)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                break\n",
    "\n",
    "        return infos\n",
    "\n",
    "    # for training policy\n",
    "    def train_one_epoch():\n",
    "\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []  # for observations\n",
    "        batch_acts = []  # for actions\n",
    "        batch_weights = []  # for R(tau) weighting in policy gradient\n",
    "        batch_rets = []  # for measuring episode returns\n",
    "        batch_lens = []  # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = train_env.reset()  # first obs comes from starting distribution\n",
    "        ep_rews = []  # list for rewards accrued throughout episode\n",
    "        ep_interest_rates = []\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "\n",
    "            # save obs\n",
    "            batch_obs.append(obs.copy()[0, :])\n",
    "\n",
    "            # act in the environment\n",
    "            act = agent.get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            act_processed = act.numpy()[0]\n",
    "\n",
    "            # collect new observation, reward, done and additional info\n",
    "            obs, rew, done, info = train_env.step(act_processed)\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew.copy())\n",
    "            ep_interest_rates.append(info['interest_rate'])\n",
    "\n",
    "            # Rollout is finished\n",
    "            if done:\n",
    "\n",
    "                # Calculate the maximum possible return of the episode.\n",
    "                # This will be reference that the agent could reach (assuming no transaction costs)\n",
    "                max_possible_return = (np.abs(np.array(ep_interest_rates)) + 1).cumprod()[-1]\n",
    "\n",
    "                # if episode is over, record info about episode\n",
    "                episode_return, episode_length = info['value'] - max_possible_return, len(ep_rews)\n",
    "\n",
    "                batch_rets.append(info['value'] - 1)\n",
    "                batch_lens.append(episode_length)\n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                batch_weights += [episode_return] * episode_length\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews = train_env.reset(), False, []\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > batch_size:\n",
    "                    break\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                                  act=torch.as_tensor(batch_acts, dtype=torch.float32),\n",
    "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32))\n",
    "\n",
    "        # calculate gradients and optimize\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return batch_loss, batch_rets, batch_lens\n",
    "\n",
    "    training_returns = list()\n",
    "    test_returns = list()\n",
    "\n",
    "    # training loop\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
    "        print('Epoch: %3d \\t Loss: %.3f \\t Return: %.3f' %\n",
    "              (i, batch_loss, np.mean(batch_rets)))\n",
    "\n",
    "        training_returns.append({\"epoch\": i, \"mean_return\": np.mean(batch_rets)})\n",
    "\n",
    "        if (i + 1) % evaluation_interval == 0:\n",
    "            def plot_results(_infos, title):\n",
    "                import cufflinks as cf\n",
    "\n",
    "                cf.go_offline()\n",
    "\n",
    "                # Create a dataframe for further processing and plotting\n",
    "                info_df = pd.DataFrame({\"info\": _infos})\n",
    "                info_df = info_df[\"info\"].apply(pd.Series).set_index(\"timestamp\")\n",
    "                info_df['value'] = info_df['value'].apply(lambda x: x.squeeze())\n",
    "                info_df['position'] = info_df['position'].apply(lambda x: x.squeeze())\n",
    "                info_df[\"cum_prod_interest_rate\"] = (info_df[\"interest_rate\"] + 1).cumprod()\n",
    "\n",
    "                info_df[[\"value\", \"position\", \"cum_prod_interest_rate\"]].iplot(secondary_y=\"position\",\n",
    "                                                                               title=title)\n",
    "\n",
    "            plot_results(evaluate(train_env), \"Evaluation on Training data\")\n",
    "            test_infos = evaluate(eval_env)\n",
    "            test_returns.append({\"epoch\": i, \"mean_return\": (test_infos[-1]['value'] - 1).squeeze()})\n",
    "            plot_results(test_infos, \"Evaluation on Test data\")\n",
    "\n",
    "    return training_returns, test_returns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define one Environment for training the agent and one environment for testing the agent on unseen data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from interest_rate_environment_pytorch import InterestEnv\n",
    "\n",
    "window_length = 6\n",
    "\n",
    "train_env_config = {\n",
    "    \"product_path\": '../data/interest_rates_p1.csv',\n",
    "    \"window_length\": window_length,\n",
    "    \"end_timestamp\": \"2020-01-01 00:00:00\"\n",
    "}\n",
    "\n",
    "eval_env_config = {\n",
    "    \"product_path\": '../data/interest_rates_p1.csv',\n",
    "    \"window_length\": window_length,\n",
    "    \"start_timestamp\": \"2020-01-01 00:00:00\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_env = InterestEnv(train_env_config)\n",
    "eval_env = InterestEnv(eval_env_config)\n",
    "agent = Agent(train_env_config['window_length'], 1)\n",
    "training_returns, test_returns = train(train_env, eval_env, agent, lr=0.01, epochs=250, batch_size=500,\n",
    "                                       evaluation_interval=50)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot Learning Progress"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = pd.DataFrame.from_records(training_returns).set_index('epoch').iplot(title='Mean Return over training epochs',\n",
    "                                                                           xTitle='Epochs',\n",
    "                                                                           yTitle='Return',\n",
    "                                                                           asFigure=True)\n",
    "fig.update_layout(yaxis=dict(tickformat=\".2%\"))\n",
    "fig.show()\n",
    "fig = pd.DataFrame.from_records(test_returns).set_index('epoch').iplot(title='Mean Return over test epochs',\n",
    "                                                                       xTitle='Epochs',\n",
    "                                                                       yTitle='Return',\n",
    "                                                                       asFigure=True)\n",
    "fig.update_layout(yaxis=dict(tickformat=\".2%\"))\n",
    "fig.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tasks\n",
    "\n",
    "1. Increase the window_length to 10. Add multiple zeros at the beginning of the weight init part. How does the agent perform now?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-346ac218",
   "language": "python",
   "display_name": "PyCharm (rl-finance-private)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}