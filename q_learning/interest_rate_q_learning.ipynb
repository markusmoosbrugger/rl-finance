{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interest Rates Reinforcement Learning: Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from interest_rate_environment import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Data Analysis\n",
    "The first step we are doing is to explore the given data and have a short look at the contents of the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_path = \"../data/interest_rates_p1.csv\"\n",
    "\n",
    "# Load the csv file with pandas\n",
    "interest_rates = pd.read_csv(product_path)\n",
    "\n",
    "# Convert column timestamp into datetime objects\n",
    "interest_rates['timestamp'] = pd.to_datetime(interest_rates[\"timestamp\"])\n",
    "\n",
    "# Calculate the avg with a rolling window\n",
    "interest_rates['interest_rate_avg'] = interest_rates['interest_rate'].rolling(window=10, center=False).mean()\n",
    "\n",
    "# Have a look at the top 10 entries\n",
    "display(interest_rates.head(10))\n",
    "\n",
    "\n",
    "# Display descriptive statistics using describe\n",
    "display(interest_rates['interest_rate'].describe())\n",
    "display(interest_rates['interest_rate_avg'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize interest rates\n",
    "In the next step we would like to visualize the given data to get an overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_rates.plot(x=\"timestamp\", y=[\"interest_rate\", \"interest_rate_avg\"], legend=True, figsize=(20,9))\n",
    "\n",
    "plt.title(\"Interest Rate Development\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create environment for Q-Learning\n",
    "\n",
    "Now we are ready to create an instance of our previously created environment and execute a view steps in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interest rate environment with the path of the csv file and optional start/end date\n",
    "env = InterestRateEnv(product_path, end = '2020-01-01')\n",
    "\n",
    "# Display statistics from interest_rate and normalized_interest_rate_avg\n",
    "display(env.df_interest_rates['interest_rate'].describe())\n",
    "display(env.df_interest_rates['normalized_interest_rate_avg'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the action and observation space\n",
    "print(\"Action Space: {}\".format(env.action_space))\n",
    "print(\"Observation Space: {}\".format(env.observation_space))\n",
    "\n",
    "# Reset environment and output first observation\n",
    "first_observation = env.reset()\n",
    "print(\"First Observation: {}\".format(first_observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute a view random actions and print observation, reward, done, info and position\n",
    "for _ in range(2):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    print(\"Action: \" + str(action))\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(\"Observation: \" +str(observation))\n",
    "    print(\"Reward: \" + str(reward))\n",
    "    print(\"Done: \" + str(done))\n",
    "    print(\"Info: \" + str(info))\n",
    "    print(\"Position: \" + str(env.current_position))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(env):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    return Q\n",
    "\n",
    "# Function that plots the development of the portfolio value within one episode\n",
    "def plot_developments(statuses):   \n",
    "    ax = statuses.plot(x=\"timestamp\", y=[\"position\", \"value\", \"cum_interest_rate\"], secondary_y=['position'], legend=True, figsize=(16,9), title=\"Performance of trained agent on evaluation data\")\n",
    "    ax.set_ylim([0.5,1.2])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def highlight_max(s): \n",
    "    is_max = s == s.max() \n",
    "    return ['background: lightgreen' if cell else '' for cell in is_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "Now we are ready to start with the actual implementation of our Q-Learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the Q-table\n",
    "print(\"Action space: \" + str(env.action_space.n))\n",
    "print(\"Observation space: \" + str(env.observation_space.n))\n",
    "Q = get_model(env)\n",
    "print(\"Q-table shape: \" + str(Q.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look how interest rates are converted to states and vice versa\n",
    "print(env.get_state_for_interest_rate(-0.97))\n",
    "print(env.get_state_for_interest_rate(0.95))\n",
    "print(env.get_interest_rate_for_state(1))\n",
    "print(env.get_interest_rate_for_state(39))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the agent\n",
    "\n",
    "def train_q_learning(env, train_episodes=100):\n",
    "        \n",
    "    # Setting the parameters for Q-Learning \n",
    "    gamma = 0.7  # discount factor\n",
    "    alpha = 0.7  # learning rate                                      \n",
    "    nr_steps = env.maximum_episode_steps()\n",
    "\n",
    "    # max_epsilon, min_epsilon and decay are used for a decayed epsilon greedy approach\n",
    "    max_epsilon = 1\n",
    "    min_epsilon = 0.01         \n",
    "    decay = 0.01\n",
    "    epsilon = max_epsilon\n",
    "    \n",
    "    # Keep track of portfolio value and epsilon in each episode\n",
    "    epsilons = []\n",
    "    portfolio_values = []\n",
    "    \n",
    "    Q = get_model(env)\n",
    "    \n",
    "    for episode in range(train_episodes):\n",
    "        # Reset the environment at every episode\n",
    "        state = env.reset()    \n",
    "\n",
    "        for step in range(nr_steps):\n",
    "            # TODO Implement Q-Learning\n",
    "\n",
    "            # Choose an action based on a random number and the given epsilon: Option 1 is to eploit, Option 2 is to explore\n",
    "\n",
    "\n",
    "            # Perform the action\n",
    "\n",
    "\n",
    "            # Update the Q-Table using the Bellman equation\n",
    "\n",
    "\n",
    "            # Update the state\n",
    "\n",
    "\n",
    "            # End the epsiode if done is true\n",
    "            if done == True:\n",
    "                break\n",
    "\n",
    "        # Decayed epsilon greedy: Cutting down on exploration by reducing the epsilon in each episode\n",
    "        # TODO (see slides for further information)\n",
    "\n",
    "        # Add the portfolio value to list of portfolio values\n",
    "        portfolio_values.append(env.current_value)\n",
    "\n",
    "        # Add epsilon to list of epsilons\n",
    "        epsilons.append(epsilon)\n",
    "\n",
    "        print(\"Finished episode {}\".format(episode))\n",
    "\n",
    "    print (\"Training score: \" + str(sum(portfolio_values)/train_episodes))\n",
    "    return Q, portfolio_values, epsilons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 200\n",
    "Q, portfolio_values, epsilons = train_q_learning(env, train_episodes=train_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display learned Q-Table\n",
    "Now we will have a look at the learned Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for Q-Table\n",
    "q_df = pd.DataFrame(Q, columns= ['SHORT', 'NO_POSITION', 'LONG'])\n",
    "q_df.reset_index(inplace=True)\n",
    "q_df['index'] = q_df['index'].apply(lambda x : env.get_interest_rate_for_state(x))\n",
    "q_df.rename(columns={'index': 'interest_rate'}, inplace=True)\n",
    "\n",
    "# Highlight highest value per row in lightgreen\n",
    "q_df = q_df.style.apply(highlight_max, axis=1, subset = q_df.columns[1:4])\n",
    "display(q_df)\n",
    "\n",
    "# Print columns with maximum value per row\n",
    "print(np.argmax(Q, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results and total portfolio values over all episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(train_episodes)\n",
    "plt.plot(x, portfolio_values)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Training portfolio value')\n",
    "plt.title('Portfolio values over all episodes') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the epsilons over all episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epsilons)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.title(\"Epsilon for episode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the agent\n",
    "Now we would like to test our reinforcement learning agent and therefore execute one episode where we always choose the \"best\" action, based on the learned Q-values.\n",
    "Think about how to split training and test data. The data which was used for the training, should not be used for test.\n",
    "Options are to specify an end date for training and a start date for test, or to use interest rates of a different product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, Q):\n",
    "    # Reset the environment\n",
    "    state = env.reset()    \n",
    "    infos = []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # No position if all Q values are equal\n",
    "        all_equal = len(set(Q[state])) <= 1\n",
    "        if all_equal:\n",
    "            action = 1\n",
    "        else:\n",
    "            # Use the \"best\" action based on the previously calculated q-table\n",
    "            action = np.argmax(Q[state,:])\n",
    "\n",
    "        # Execute one step\n",
    "        state, reward, done, info = env.step(action)\n",
    "        infos.append(info)\n",
    "\n",
    "        # End the episode if done = True\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "    print(\"Final portfolio value: \" + str(env.current_value))\n",
    "    return infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the agent\n",
    "env = InterestRateEnv(product_path, start= '2020-01-01')\n",
    "infos = test_agent(env, Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display portfolio development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame with all the statues\n",
    "info_df = pd.DataFrame.from_dict(infos)\n",
    "info_df[\"cum_interest_rate\"] = (info_df[\"interest_rate\"] + 1).cumprod()\n",
    "\n",
    "print(info_df[:5])\n",
    "\n",
    "# Plot the portfolio developments and the positions over the testing time\n",
    "plot_developments(info_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-rl-finance",
   "language": "python",
   "name": "venv-rl-finance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
